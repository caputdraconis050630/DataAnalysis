{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 1px solid #455A64;background-color:#ECEFF1;\">\n",
    "본 자료 및 영상 컨텐츠는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 컨텐츠 및 컨텐츠 일부 문구등을 외부에 공개, 게시하는 것을 금지합니다. 특히 자료에 대해서는 저작권법을 엄격하게 적용하겠습니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 준비 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open('titanic_step4_importance_train.pickle', 'rb') as pickle_filename:\n",
    "    train_importance = pickle.load(pickle_filename)\n",
    "with open('titanic_step4_importance_test.pickle', 'rb') as pickle_filename:\n",
    "    test_importance = pickle.load(pickle_filename)\n",
    "with open('titanic_step4_importance_train_y.pickle', 'rb') as pickle_filename:\n",
    "    train_answer = pickle.load(pickle_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주요 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # 각 모델에서 내부적으로 관련 라이브러리 사용 가능\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier             # 1. K-Nearest Neighbor(KNN)\n",
    "from sklearn.linear_model import LogisticRegression            # 2. Logistic Regression\n",
    "from sklearn.svm import SVC                                                # 3. SVC\n",
    "from sklearn.tree import DecisionTreeClassifier                   # 4. Decision Tree\n",
    "from sklearn.ensemble import RandomForestClassifier       # 5. Random Forest\n",
    "from sklearn.ensemble import ExtraTreesClassifier             # 6. Extra Tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier  # 7. GBM\n",
    "from sklearn.naive_bayes import GaussianNB                     # 8. GaussianNB\n",
    "from xgboost import XGBClassifier                                     # 9. XGBoost\n",
    "from lightgbm import LGBMClassifier                                 # 10. LightGBM\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼 파라미터 튜닝과 SVC\n",
    "- SVC 는 결국 분류의 경계가 되는 경계선을 작성하여, 분류를 실행하는 모델\n",
    "- 해당 경계선을 일직선으로 할지, 어느 정도 곡률을 가진 선으로 할지도 선정 가능\n",
    "- 주요 하이퍼 파라미터\n",
    "  - C : regularization 파라미터\n",
    "  - gamma: 어느 정도 훈련 셋에 fit 하게 할지를 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV + SVC\n",
    "- RandomizedSearchCV() 는 랜덤하게 파라미터값을 선정하여, 테스트를 수행하므로,\n",
    "- 보다 적합한 파라미터값을 도출하기 위해서는 수행횟수를 늘려야 함\n",
    "- 따라서 머신러닝 모델의 수행성능에 따라, RandomizedSearchCV() 사용시, 수행시간이 상당히 오래 걸릴 수 있으므로,\n",
    "- RandomizedSearchCV() 이해를 위해서만 일부 모델에서만 테스트를 진행하고,\n",
    "- GridSearchCV() 을 주로 사용하기로 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고: 균일 분포 또는 균등 분포(Uniform Distribution)\n",
    "  - 정해진 범위에서 모든 확률이 균일한 분포를 의미함\n",
    "  - 균일 분포는 이산형 확률 분포와 연속형 확률 분포 두 형태가 존재 \n",
    "  - 연속형 확률 분포: 두 점 a,b 사이의 연속적인 값에 대한 확률 분포\n",
    "  - 이산형 확률 분포: 두 점 a,b 사이에 갯수가 정해진 값들에 대한 확률 분포"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stats.uniform(loc, scale)\n",
    "- loc 부터, loc + scale 까지의 범위에서 균등한 확률로 연속형 값을 추출\n",
    "- 해당 객체는 rvs() 메서드를 가지고 있고, 이를 사용해서, RandomizedSearchCV() 가 랜덤 값을 균등 확률로 추출해서, 적용 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8372920720607621\n",
      "{'C': 7.337794540855652, 'gamma': 0.0923385947687978}\n",
      "                                               params  mean_test_score\n",
      "0   {'C': 20.8511002351287, 'gamma': 0.72032449344...         0.822717\n",
      "1   {'C': 0.005718740867244332, 'gamma': 0.3023325...         0.616163\n",
      "2   {'C': 7.337794540855652, 'gamma': 0.0923385947...         0.837292\n",
      "3   {'C': 9.313010568883545, 'gamma': 0.3455607270...         0.823809\n",
      "4   {'C': 19.838373711533496, 'gamma': 0.538816734...         0.823821\n",
      "..                                                ...              ...\n",
      "95  {'C': 13.164838524355549, 'gamma': 0.065961090...         0.832804\n",
      "96  {'C': 36.753298164433474, 'gamma': 0.772178029...         0.818235\n",
      "97  {'C': 45.3907926251762, 'gamma': 0.93197206919...         0.812623\n",
      "98  {'C': 0.6975786487798508, 'gamma': 0.234362086...         0.835013\n",
      "99  {'C': 30.83891785008288, 'gamma': 0.9490163206...         0.811506\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    \"C\": stats.uniform(0, 50),\n",
    "    \"gamma\": stats.uniform(0, 1)\n",
    "}\n",
    "gd = RandomizedSearchCV(\n",
    "    estimator = SVC(random_state=1), \n",
    "    param_distributions=hyperparams, \n",
    "    n_iter=100, \n",
    "    cv=5,   # 내부적으로 (Stratified)KFold 사용\n",
    "    scoring='accuracy', \n",
    "    random_state=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "gd.fit(train_importance, train_answer)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)\n",
    "\n",
    "df = pd.DataFrame(gd.cv_results_)\n",
    "print(df[['params','mean_test_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC + 하이퍼 파라미터 튜닝 (GridSearchCV 사용)\n",
    "> 하이퍼파라미터는 일반적으로 적절한 범위가 없기 때문에, 각 데이터에 맞춰서 성능이 나오는 범위를 감으로 지정해야 함\n",
    "> RandomizedSearchCV() 를 통해, 대략적인 범위를 알아낸 후, 이를 기반으로 GridSearchCV() 를 사용하여\n",
    "> 범위와, 최적의 값을 가질 수 있는 후보군을 지정하는 방식으로, 최적의 하이퍼파라미터 값을 찾아가는 방법을 많이 사용함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    1.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8395016006528152\n",
      "{'C': 50, 'gamma': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 179 out of 210 | elapsed:    2.5s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed:    2.6s finished\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 그리드 셋팅\n",
    "hyperparams = {\n",
    "    'C': [10, 15, 20, 23, 25, 30, 50], \n",
    "    'gamma' : [0.001, 0.01, 0.05, 0.06, 0.07, 0.1]\n",
    "}\n",
    "\n",
    "# 교차검증\n",
    "gd=GridSearchCV(\n",
    "    estimator = SVC(random_state=1), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5,   # 내부적으로 (Stratified)KFold 사용\n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 모델 fiting 및 결과\n",
    "gd.fit(train_importance, train_answer)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #E65100;background-color:#FFF3E0;padding:10px\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#BF360C;\">큰그림으로 이해하기</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">데이터를 어떻게 짜르느냐에 따라, 예측 성능은 다르게 계산될 수 있지만,</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">동일한 테스트 환경에서, 하이퍼파라미터 튜닝 전 보다, PC에 따라 다르겠지만, 다소 예측 정확도가 올라감</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient Boosting Classifier 주요 하이퍼 파라미터\n",
    "- learning_rate는 학습률을 의미하며, 각 트리의 오류에 기반해서, 어느 정도 수정할지의 비율을 의미\n",
    "- n_estimator 는 트리의 갯수를 의미\n",
    "- max_depth 는 트리의 깊이를 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier + 하이퍼 파라미터 튜닝 (GridSearchCV 사용)\n",
    "- Gradient Boosting Classifier 는 수행시간이 오래 걸리므로, RandomizedSearchCV() 로 사전 테스트를 통해, 대략적인 최적의 파라미터값을 예상하여, GridSearchCV() 로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8417676228736426\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  2.1min finished\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.01, 0.05, 0.1, 0.2]\n",
    "n_estimators = [100, 1000, 2000]\n",
    "max_depth = [3, 5, 10, 15]\n",
    "\n",
    "hyperparams = {\n",
    "    'learning_rate': learning_rate, \n",
    "    'n_estimators': n_estimators, \n",
    "    'max_depth': max_depth\n",
    "}\n",
    "\n",
    "gd=GridSearchCV(\n",
    "    estimator = GradientBoostingClassifier(random_state=1), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5, \n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gd.fit(train_importance, train_answer)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #E65100;background-color:#FFF3E0;padding:10px\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#BF360C;\">큰그림으로 이해하기</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">데이터를 어떻게 짜르느냐에 따라, 예측 성능은 다르게 계산될 수 있지만,</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">동일한 테스트 환경에서, 하이퍼파라미터 튜닝 전 보다, 성능이 개선됨</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Logistic Regression 주요 하이퍼 파라미터\n",
    "* penalty: regularization 종류 선정 (l1, l2 등)\n",
    "* C: regularization 적용 강도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression + 하이퍼 파라미터 튜닝 (RandomizedSearchCV 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8395455401418618\n",
      "{'C': 18.288277344191805, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# 본 코드는 컴퓨터 성능에 따라 수행시간이 매우 오래 걸리고, 수행시간 제한으로 주피터 노트북등이 다운될 수도 있음\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "hyperparams = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'], \n",
    "    'C': stats.uniform(0, 1000)\n",
    "}\n",
    "gd = RandomizedSearchCV(\n",
    "    estimator = LogisticRegression(random_state=1), \n",
    "    param_distributions=hyperparams, \n",
    "    n_iter=100, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    random_state=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "gd.fit(train_importance, train_answer)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression + 하이퍼 파라미터 튜닝 (GridSearchCV 사용)\n",
    "- numpy.linspace(start, end, num)\n",
    "  - start ~ end 사이의 값을 등간격으로 num 갯수만큼의 배열을 생성하는 numpy 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([700.        , 722.22222222, 744.44444444, 766.66666667,\n",
       "       788.88888889, 811.11111111, 833.33333333, 855.55555556,\n",
       "       877.77777778, 900.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(700, 900, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 400 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 800 tasks      | elapsed:    1.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83841566756638\n",
      "{'C': 720.1005025125628, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 2000 out of 2000 | elapsed:    3.7s finished\n"
     ]
    }
   ],
   "source": [
    "penalty = ['l1', 'l2']\n",
    "C = np.linspace(700, 900, 200)\n",
    "\n",
    "hyperparams = {\n",
    "    'penalty': penalty, \n",
    "    'C': C\n",
    "}\n",
    "\n",
    "gd=GridSearchCV(\n",
    "    estimator = LogisticRegression(random_state=1), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5, \n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gd.fit(train_importance, train_answer)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #E65100;background-color:#FFF3E0;padding:10px\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#BF360C;\">큰그림으로 이해하기</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">데이터를 어떻게 짜르느냐에 따라, 예측 성능은 다르게 계산될 수 있으며,</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">동일한 테스트 환경에서, 하이퍼파라미터 튜닝 전 보다는 정확도가 올라감</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. XGBoost 와 주요 하이퍼 파라미터\n",
    "- 일반적인 XGBoost 하이퍼 파라미터 튜닝 전략\n",
    "   - ensemble 방식은 수행시, 각 파라미터를 적절히 맞춰주기 때문에, 하이퍼 파라미터 튜닝이 정확도를 높이는데 있어서, 큰 기여를 하는 편은 아니며, \n",
    "   - 무수히 많은 파라미터가 있고, 수행 시간이 오래 걸리므로, 주요한 파라미터들만 중심으로 튜닝을 진행하는 편이 좋음\n",
    "- 성능에 영향을 많이 끼치는 주요 파라미터\n",
    "   - learning_rate \n",
    "      - 이전 결과를 얼마나 반영할지에 대한 학습 단계별 적용할 가중치를 의미함 \n",
    "      - 일반적으로 0.01 ~ 0.2 사이의 값을 많이 사용함\n",
    "   - max_depth\n",
    "      - 트리의 최대 깊이를 의미함\n",
    "      - 트리의 최대 깊이로 -1 로 하면, 깊이에 제한을 두지 않음 \n",
    "      - 일반적으로 3 ~ 10 사이의 값을 많이 사용함\n",
    "   - gamma\n",
    "      - 일종의 정규화(regularization) 파라미터로, gamma 가 높을 수록, regularization 이 높다고 이해하면 됨\n",
    "      - 트리에서 가지를 추가로 만들기 위해 필요한 최소 loss 기준값으로, gamma 값이 작으면, 트리에 보다 많은 가지가 만들어진다고 이해하면 됨\n",
    "      - 일반적으로 0 이상의 값을 가짐\n",
    "   - min_child_weight\n",
    "      - 트리에서 가지를 추가로 만들어 분할하기 위해, 필요한 최소한의 샘플 수\n",
    "      - 값이 적을 수록, 트리가 더 분할될 수 있음\n",
    "      - 일반적으로 0 이상의 값을 가짐\n",
    "   - subsample\n",
    "      - 각 트리마다 모든 훈련데이터를 사용해서 트리를 만들지 않음\n",
    "      - 훈련 데이터의 일부를 사용해서 트리를 만든다면, 보다 많은 트리를 만들 수 있고, 이를 통해 트리의 다양성을 높일 수 있음\n",
    "      - subsample 은 이 때, 각 트리마다 어느 정도의 훈련 데이터 비율을 사용해서 트리를 만들지를 그 비율을 정하는 것임\n",
    "      - 일반적으로 0.5 ~ 1 사이의 값을 많이 사용함\n",
    "   - colsample_bytree\n",
    "      - subsample 과 마찬가지로, 훈련 데이터에서 일부 feature (컬럼) 들만 뽑아서 트리를 만드는 방식\n",
    "      - 일부 feature (컬럼)들만 뽑아서 트리를 만든다면, 보다 많은 트리를 만들 수 있고, 이를 통해 트리의 다양성을 높일 수 있음\n",
    "      - colsample_bytree 는 이를 위해 각 트리를 만들 때 사용할 feature의 비율을 정하는 것임\n",
    "      - 일반적으로 0.5 ~ 1 사이의 값을 많이 사용함\n",
    "      \n",
    "   - reg_alpha: L1 정규화(regularization) 가중치\n",
    "   - reg_lambda: L2 정규화(regularization) 가중치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization 를 위한 파이썬 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bayesian-optimization in /Users/davelee/opt/anaconda3/lib/python3.8/site-packages (1.2.0)\r\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /Users/davelee/opt/anaconda3/lib/python3.8/site-packages (from bayesian-optimization) (0.23.2)\r\n",
      "Requirement already satisfied: numpy>=1.9.0 in /Users/davelee/opt/anaconda3/lib/python3.8/site-packages (from bayesian-optimization) (1.19.2)\r\n",
      "Requirement already satisfied: scipy>=0.14.0 in /Users/davelee/opt/anaconda3/lib/python3.8/site-packages (from bayesian-optimization) (1.5.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/davelee/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.17.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/davelee/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (2.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization XGBoost 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | learni... | max_depth | min_ch... | n_esti... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8226  \u001b[0m | \u001b[0m 0.7085  \u001b[0m | \u001b[0m 3.602   \u001b[0m | \u001b[0m 0.01006 \u001b[0m | \u001b[0m 5.116   \u001b[0m | \u001b[0m 1.468   \u001b[0m | \u001b[0m 183.1   \u001b[0m | \u001b[0m 0.5931  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8261  \u001b[0m | \u001b[95m 0.6728  \u001b[0m | \u001b[95m 1.984   \u001b[0m | \u001b[95m 0.274   \u001b[0m | \u001b[95m 5.934   \u001b[0m | \u001b[95m 6.852   \u001b[0m | \u001b[95m 284.0   \u001b[0m | \u001b[95m 0.9391  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.8361  \u001b[0m | \u001b[95m 0.5137  \u001b[0m | \u001b[95m 3.352   \u001b[0m | \u001b[95m 0.2145  \u001b[0m | \u001b[95m 6.911   \u001b[0m | \u001b[95m 1.404   \u001b[0m | \u001b[95m 278.3   \u001b[0m | \u001b[95m 0.9004  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.807   \u001b[0m | \u001b[0m 0.9841  \u001b[0m | \u001b[0m 1.567   \u001b[0m | \u001b[0m 0.3492  \u001b[0m | \u001b[0m 9.135   \u001b[0m | \u001b[0m 8.946   \u001b[0m | \u001b[0m 176.5   \u001b[0m | \u001b[0m 0.5195  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8092  \u001b[0m | \u001b[0m 0.5849  \u001b[0m | \u001b[0m 4.391   \u001b[0m | \u001b[0m 0.05819 \u001b[0m | \u001b[0m 5.948   \u001b[0m | \u001b[0m 9.579   \u001b[0m | \u001b[0m 579.8   \u001b[0m | \u001b[0m 0.8459  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8182  \u001b[0m | \u001b[0m 0.6578  \u001b[0m | \u001b[0m 3.433   \u001b[0m | \u001b[0m 0.419   \u001b[0m | \u001b[0m 3.128   \u001b[0m | \u001b[0m 7.501   \u001b[0m | \u001b[0m 990.0   \u001b[0m | \u001b[0m 0.8741  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8103  \u001b[0m | \u001b[0m 0.6402  \u001b[0m | \u001b[0m 3.946   \u001b[0m | \u001b[0m 0.06058 \u001b[0m | \u001b[0m 6.135   \u001b[0m | \u001b[0m 9.086   \u001b[0m | \u001b[0m 364.3   \u001b[0m | \u001b[0m 0.6439  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8193  \u001b[0m | \u001b[0m 0.565   \u001b[0m | \u001b[0m 0.09683 \u001b[0m | \u001b[0m 0.3426  \u001b[0m | \u001b[0m 4.481   \u001b[0m | \u001b[0m 2.655   \u001b[0m | \u001b[0m 542.4   \u001b[0m | \u001b[0m 0.5267  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8328  \u001b[0m | \u001b[0m 0.7871  \u001b[0m | \u001b[0m 0.7336  \u001b[0m | \u001b[0m 0.2988  \u001b[0m | \u001b[0m 7.898   \u001b[0m | \u001b[0m 1.023   \u001b[0m | \u001b[0m 472.7   \u001b[0m | \u001b[0m 0.8472  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8205  \u001b[0m | \u001b[0m 0.7071  \u001b[0m | \u001b[0m 0.2498  \u001b[0m | \u001b[0m 0.2726  \u001b[0m | \u001b[0m 7.647   \u001b[0m | \u001b[0m 5.149   \u001b[0m | \u001b[0m 950.1   \u001b[0m | \u001b[0m 0.7933  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8316  \u001b[0m | \u001b[0m 0.7806  \u001b[0m | \u001b[0m 2.368   \u001b[0m | \u001b[0m 0.2536  \u001b[0m | \u001b[0m 6.873   \u001b[0m | \u001b[0m 1.719   \u001b[0m | \u001b[0m 277.6   \u001b[0m | \u001b[0m 0.993   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8216  \u001b[0m | \u001b[0m 0.5356  \u001b[0m | \u001b[0m 4.557   \u001b[0m | \u001b[0m 0.3177  \u001b[0m | \u001b[0m 4.228   \u001b[0m | \u001b[0m 0.5913  \u001b[0m | \u001b[0m 279.8   \u001b[0m | \u001b[0m 0.7951  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.826   \u001b[0m | \u001b[0m 0.8655  \u001b[0m | \u001b[0m 3.52    \u001b[0m | \u001b[0m 0.2596  \u001b[0m | \u001b[0m 8.443   \u001b[0m | \u001b[0m 1.127   \u001b[0m | \u001b[0m 279.1   \u001b[0m | \u001b[0m 0.7899  \u001b[0m |\n",
      "| \u001b[95m 14      \u001b[0m | \u001b[95m 0.8395  \u001b[0m | \u001b[95m 0.9958  \u001b[0m | \u001b[95m 1.608   \u001b[0m | \u001b[95m 0.1218  \u001b[0m | \u001b[95m 6.166   \u001b[0m | \u001b[95m 0.3382  \u001b[0m | \u001b[95m 473.5   \u001b[0m | \u001b[95m 0.8209  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.8395  \u001b[0m | \u001b[0m 0.5427  \u001b[0m | \u001b[0m 3.394   \u001b[0m | \u001b[0m 0.09469 \u001b[0m | \u001b[0m 7.127   \u001b[0m | \u001b[0m 0.2307  \u001b[0m | \u001b[0m 472.5   \u001b[0m | \u001b[0m 0.8964  \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.835   \u001b[0m | \u001b[0m 0.9214  \u001b[0m | \u001b[0m 3.46    \u001b[0m | \u001b[0m 0.1568  \u001b[0m | \u001b[0m 5.097   \u001b[0m | \u001b[0m 1.157   \u001b[0m | \u001b[0m 473.7   \u001b[0m | \u001b[0m 0.7926  \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.8339  \u001b[0m | \u001b[0m 0.9265  \u001b[0m | \u001b[0m 2.118   \u001b[0m | \u001b[0m 0.2849  \u001b[0m | \u001b[0m 4.31    \u001b[0m | \u001b[0m 0.1267  \u001b[0m | \u001b[0m 470.2   \u001b[0m | \u001b[0m 0.5167  \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.835   \u001b[0m | \u001b[0m 0.6561  \u001b[0m | \u001b[0m 3.93    \u001b[0m | \u001b[0m 0.3829  \u001b[0m | \u001b[0m 8.193   \u001b[0m | \u001b[0m 0.2576  \u001b[0m | \u001b[0m 475.6   \u001b[0m | \u001b[0m 0.8075  \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.8373  \u001b[0m | \u001b[0m 0.9031  \u001b[0m | \u001b[0m 4.363   \u001b[0m | \u001b[0m 0.3519  \u001b[0m | \u001b[0m 7.172   \u001b[0m | \u001b[0m 1.832   \u001b[0m | \u001b[0m 469.4   \u001b[0m | \u001b[0m 0.6135  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.8261  \u001b[0m | \u001b[0m 0.6726  \u001b[0m | \u001b[0m 3.293   \u001b[0m | \u001b[0m 0.4648  \u001b[0m | \u001b[0m 8.793   \u001b[0m | \u001b[0m 4.908   \u001b[0m | \u001b[0m 471.3   \u001b[0m | \u001b[0m 0.744   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.826   \u001b[0m | \u001b[0m 0.9255  \u001b[0m | \u001b[0m 4.959   \u001b[0m | \u001b[0m 0.167   \u001b[0m | \u001b[0m 9.343   \u001b[0m | \u001b[0m 0.483   \u001b[0m | \u001b[0m 469.2   \u001b[0m | \u001b[0m 0.9781  \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.8283  \u001b[0m | \u001b[0m 0.5303  \u001b[0m | \u001b[0m 3.154   \u001b[0m | \u001b[0m 0.445   \u001b[0m | \u001b[0m 4.831   \u001b[0m | \u001b[0m 3.229   \u001b[0m | \u001b[0m 470.0   \u001b[0m | \u001b[0m 0.7915  \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.8339  \u001b[0m | \u001b[0m 0.8236  \u001b[0m | \u001b[0m 0.2179  \u001b[0m | \u001b[0m 0.02019 \u001b[0m | \u001b[0m 3.123   \u001b[0m | \u001b[0m 0.2025  \u001b[0m | \u001b[0m 475.7   \u001b[0m | \u001b[0m 0.9424  \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.8384  \u001b[0m | \u001b[0m 0.7933  \u001b[0m | \u001b[0m 1.766   \u001b[0m | \u001b[0m 0.4589  \u001b[0m | \u001b[0m 5.781   \u001b[0m | \u001b[0m 2.352   \u001b[0m | \u001b[0m 477.3   \u001b[0m | \u001b[0m 0.7455  \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.825   \u001b[0m | \u001b[0m 0.9726  \u001b[0m | \u001b[0m 0.4691  \u001b[0m | \u001b[0m 0.2322  \u001b[0m | \u001b[0m 7.745   \u001b[0m | \u001b[0m 1.636   \u001b[0m | \u001b[0m 477.8   \u001b[0m | \u001b[0m 0.7934  \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.835   \u001b[0m | \u001b[0m 0.7014  \u001b[0m | \u001b[0m 2.772   \u001b[0m | \u001b[0m 0.09948 \u001b[0m | \u001b[0m 3.717   \u001b[0m | \u001b[0m 2.366   \u001b[0m | \u001b[0m 477.6   \u001b[0m | \u001b[0m 0.9362  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.8227  \u001b[0m | \u001b[0m 0.8666  \u001b[0m | \u001b[0m 4.292   \u001b[0m | \u001b[0m 0.1412  \u001b[0m | \u001b[0m 6.842   \u001b[0m | \u001b[0m 3.708   \u001b[0m | \u001b[0m 476.9   \u001b[0m | \u001b[0m 0.6627  \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.8328  \u001b[0m | \u001b[0m 0.9057  \u001b[0m | \u001b[0m 1.33    \u001b[0m | \u001b[0m 0.196   \u001b[0m | \u001b[0m 4.252   \u001b[0m | \u001b[0m 2.47    \u001b[0m | \u001b[0m 475.4   \u001b[0m | \u001b[0m 0.6511  \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.8294  \u001b[0m | \u001b[0m 0.7747  \u001b[0m | \u001b[0m 0.7298  \u001b[0m | \u001b[0m 0.1216  \u001b[0m | \u001b[0m 3.952   \u001b[0m | \u001b[0m 2.415   \u001b[0m | \u001b[0m 479.1   \u001b[0m | \u001b[0m 0.9758  \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.8339  \u001b[0m | \u001b[0m 0.8885  \u001b[0m | \u001b[0m 2.9     \u001b[0m | \u001b[0m 0.1618  \u001b[0m | \u001b[0m 5.135   \u001b[0m | \u001b[0m 0.005012\u001b[0m | \u001b[0m 478.4   \u001b[0m | \u001b[0m 0.8577  \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.835   \u001b[0m | \u001b[0m 0.6531  \u001b[0m | \u001b[0m 4.564   \u001b[0m | \u001b[0m 0.2384  \u001b[0m | \u001b[0m 5.027   \u001b[0m | \u001b[0m 0.01396 \u001b[0m | \u001b[0m 469.8   \u001b[0m | \u001b[0m 0.5355  \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.8361  \u001b[0m | \u001b[0m 0.8232  \u001b[0m | \u001b[0m 3.211   \u001b[0m | \u001b[0m 0.1435  \u001b[0m | \u001b[0m 6.551   \u001b[0m | \u001b[0m 0.9526  \u001b[0m | \u001b[0m 466.4   \u001b[0m | \u001b[0m 0.7053  \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 0.8283  \u001b[0m | \u001b[0m 0.855   \u001b[0m | \u001b[0m 1.236   \u001b[0m | \u001b[0m 0.3912  \u001b[0m | \u001b[0m 7.602   \u001b[0m | \u001b[0m 1.704   \u001b[0m | \u001b[0m 467.9   \u001b[0m | \u001b[0m 0.8947  \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.8283  \u001b[0m | \u001b[0m 0.9841  \u001b[0m | \u001b[0m 4.813   \u001b[0m | \u001b[0m 0.06945 \u001b[0m | \u001b[0m 4.901   \u001b[0m | \u001b[0m 1.717   \u001b[0m | \u001b[0m 464.1   \u001b[0m | \u001b[0m 0.662   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.816   \u001b[0m | \u001b[0m 0.5059  \u001b[0m | \u001b[0m 0.2367  \u001b[0m | \u001b[0m 0.4134  \u001b[0m | \u001b[0m 3.087   \u001b[0m | \u001b[0m 0.8116  \u001b[0m | \u001b[0m 464.6   \u001b[0m | \u001b[0m 0.5977  \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m 0.8249  \u001b[0m | \u001b[0m 0.6367  \u001b[0m | \u001b[0m 4.616   \u001b[0m | \u001b[0m 0.03537 \u001b[0m | \u001b[0m 6.099   \u001b[0m | \u001b[0m 1.34    \u001b[0m | \u001b[0m 276.5   \u001b[0m | \u001b[0m 0.5355  \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 0.8272  \u001b[0m | \u001b[0m 0.8611  \u001b[0m | \u001b[0m 2.594   \u001b[0m | \u001b[0m 0.4766  \u001b[0m | \u001b[0m 4.396   \u001b[0m | \u001b[0m 0.1375  \u001b[0m | \u001b[0m 476.3   \u001b[0m | \u001b[0m 0.7798  \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.8339  \u001b[0m | \u001b[0m 0.6327  \u001b[0m | \u001b[0m 2.417   \u001b[0m | \u001b[0m 0.05884 \u001b[0m | \u001b[0m 5.387   \u001b[0m | \u001b[0m 0.818   \u001b[0m | \u001b[0m 471.7   \u001b[0m | \u001b[0m 0.684   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m 0.8316  \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 2.566   \u001b[0m | \u001b[0m 0.2667  \u001b[0m | \u001b[0m 6.776   \u001b[0m | \u001b[0m 1.282   \u001b[0m | \u001b[0m 473.4   \u001b[0m | \u001b[0m 0.5594  \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 0.8305  \u001b[0m | \u001b[0m 0.8966  \u001b[0m | \u001b[0m 4.597   \u001b[0m | \u001b[0m 0.3948  \u001b[0m | \u001b[0m 6.559   \u001b[0m | \u001b[0m 0.132   \u001b[0m | \u001b[0m 470.6   \u001b[0m | \u001b[0m 0.9534  \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m 0.8238  \u001b[0m | \u001b[0m 0.6134  \u001b[0m | \u001b[0m 3.988   \u001b[0m | \u001b[0m 0.03613 \u001b[0m | \u001b[0m 7.375   \u001b[0m | \u001b[0m 3.376   \u001b[0m | \u001b[0m 467.8   \u001b[0m | \u001b[0m 0.5343  \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 0.8283  \u001b[0m | \u001b[0m 0.8502  \u001b[0m | \u001b[0m 4.53    \u001b[0m | \u001b[0m 0.3496  \u001b[0m | \u001b[0m 5.178   \u001b[0m | \u001b[0m 0.07869 \u001b[0m | \u001b[0m 472.7   \u001b[0m | \u001b[0m 0.9822  \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 0.8316  \u001b[0m | \u001b[0m 0.5167  \u001b[0m | \u001b[0m 3.892   \u001b[0m | \u001b[0m 0.4346  \u001b[0m | \u001b[0m 6.493   \u001b[0m | \u001b[0m 0.4789  \u001b[0m | \u001b[0m 468.4   \u001b[0m | \u001b[0m 0.8143  \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 0.8294  \u001b[0m | \u001b[0m 0.672   \u001b[0m | \u001b[0m 4.854   \u001b[0m | \u001b[0m 0.3859  \u001b[0m | \u001b[0m 7.991   \u001b[0m | \u001b[0m 0.6038  \u001b[0m | \u001b[0m 473.2   \u001b[0m | \u001b[0m 0.8308  \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 0.8249  \u001b[0m | \u001b[0m 0.5699  \u001b[0m | \u001b[0m 0.228   \u001b[0m | \u001b[0m 0.2124  \u001b[0m | \u001b[0m 5.977   \u001b[0m | \u001b[0m 1.034   \u001b[0m | \u001b[0m 474.3   \u001b[0m | \u001b[0m 0.9092  \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m 0.8328  \u001b[0m | \u001b[0m 0.6655  \u001b[0m | \u001b[0m 3.45    \u001b[0m | \u001b[0m 0.2798  \u001b[0m | \u001b[0m 6.637   \u001b[0m | \u001b[0m 1.699   \u001b[0m | \u001b[0m 478.9   \u001b[0m | \u001b[0m 0.9913  \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m 0.8339  \u001b[0m | \u001b[0m 0.5418  \u001b[0m | \u001b[0m 2.382   \u001b[0m | \u001b[0m 0.09239 \u001b[0m | \u001b[0m 4.871   \u001b[0m | \u001b[0m 1.488   \u001b[0m | \u001b[0m 478.4   \u001b[0m | \u001b[0m 0.6471  \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 0.8306  \u001b[0m | \u001b[0m 0.5705  \u001b[0m | \u001b[0m 1.436   \u001b[0m | \u001b[0m 0.2464  \u001b[0m | \u001b[0m 4.865   \u001b[0m | \u001b[0m 0.03687 \u001b[0m | \u001b[0m 472.3   \u001b[0m | \u001b[0m 0.5717  \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m 0.8294  \u001b[0m | \u001b[0m 0.6238  \u001b[0m | \u001b[0m 4.905   \u001b[0m | \u001b[0m 0.4767  \u001b[0m | \u001b[0m 6.431   \u001b[0m | \u001b[0m 2.394   \u001b[0m | \u001b[0m 470.5   \u001b[0m | \u001b[0m 0.5125  \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 0.8373  \u001b[0m | \u001b[0m 0.7009  \u001b[0m | \u001b[0m 2.168   \u001b[0m | \u001b[0m 0.09034 \u001b[0m | \u001b[0m 7.166   \u001b[0m | \u001b[0m 0.1781  \u001b[0m | \u001b[0m 471.0   \u001b[0m | \u001b[0m 0.7578  \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 0.8272  \u001b[0m | \u001b[0m 0.7475  \u001b[0m | \u001b[0m 3.342   \u001b[0m | \u001b[0m 0.4823  \u001b[0m | \u001b[0m 8.725   \u001b[0m | \u001b[0m 1.476   \u001b[0m | \u001b[0m 471.0   \u001b[0m | \u001b[0m 0.8526  \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 0.8373  \u001b[0m | \u001b[0m 0.8973  \u001b[0m | \u001b[0m 1.218   \u001b[0m | \u001b[0m 0.3191  \u001b[0m | \u001b[0m 6.795   \u001b[0m | \u001b[0m 0.02747 \u001b[0m | \u001b[0m 471.9   \u001b[0m | \u001b[0m 0.9229  \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 0.8328  \u001b[0m | \u001b[0m 0.5439  \u001b[0m | \u001b[0m 1.75    \u001b[0m | \u001b[0m 0.4926  \u001b[0m | \u001b[0m 3.98    \u001b[0m | \u001b[0m 4.475   \u001b[0m | \u001b[0m 476.7   \u001b[0m | \u001b[0m 0.5827  \u001b[0m |\n",
      "| \u001b[95m 54      \u001b[0m | \u001b[95m 0.8406  \u001b[0m | \u001b[95m 0.7307  \u001b[0m | \u001b[95m 2.301   \u001b[0m | \u001b[95m 0.3241  \u001b[0m | \u001b[95m 7.064   \u001b[0m | \u001b[95m 0.1876  \u001b[0m | \u001b[95m 474.4   \u001b[0m | \u001b[95m 0.6592  \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 0.8361  \u001b[0m | \u001b[0m 0.8131  \u001b[0m | \u001b[0m 4.394   \u001b[0m | \u001b[0m 0.1639  \u001b[0m | \u001b[0m 7.353   \u001b[0m | \u001b[0m 0.05145 \u001b[0m | \u001b[0m 480.9   \u001b[0m | \u001b[0m 0.9036  \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m 0.8294  \u001b[0m | \u001b[0m 0.7937  \u001b[0m | \u001b[0m 4.835   \u001b[0m | \u001b[0m 0.1503  \u001b[0m | \u001b[0m 5.262   \u001b[0m | \u001b[0m 0.1787  \u001b[0m | \u001b[0m 480.2   \u001b[0m | \u001b[0m 0.7336  \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m 0.8294  \u001b[0m | \u001b[0m 0.5121  \u001b[0m | \u001b[0m 3.274   \u001b[0m | \u001b[0m 0.3831  \u001b[0m | \u001b[0m 8.224   \u001b[0m | \u001b[0m 0.4002  \u001b[0m | \u001b[0m 482.7   \u001b[0m | \u001b[0m 0.8017  \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m 0.8361  \u001b[0m | \u001b[0m 0.8199  \u001b[0m | \u001b[0m 2.81    \u001b[0m | \u001b[0m 0.3896  \u001b[0m | \u001b[0m 7.551   \u001b[0m | \u001b[0m 0.2244  \u001b[0m | \u001b[0m 464.9   \u001b[0m | \u001b[0m 0.6136  \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m 0.8395  \u001b[0m | \u001b[0m 0.9626  \u001b[0m | \u001b[0m 3.691   \u001b[0m | \u001b[0m 0.3361  \u001b[0m | \u001b[0m 9.8     \u001b[0m | \u001b[0m 0.1362  \u001b[0m | \u001b[0m 479.8   \u001b[0m | \u001b[0m 0.8567  \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m 0.8261  \u001b[0m | \u001b[0m 0.9618  \u001b[0m | \u001b[0m 4.472   \u001b[0m | \u001b[0m 0.4247  \u001b[0m | \u001b[0m 9.053   \u001b[0m | \u001b[0m 1.354   \u001b[0m | \u001b[0m 478.6   \u001b[0m | \u001b[0m 0.6904  \u001b[0m |\n",
      "| \u001b[0m 61      \u001b[0m | \u001b[0m 0.8339  \u001b[0m | \u001b[0m 0.733   \u001b[0m | \u001b[0m 2.166   \u001b[0m | \u001b[0m 0.03521 \u001b[0m | \u001b[0m 8.155   \u001b[0m | \u001b[0m 0.3637  \u001b[0m | \u001b[0m 479.7   \u001b[0m | \u001b[0m 0.6384  \u001b[0m |\n",
      "| \u001b[0m 62      \u001b[0m | \u001b[0m 0.835   \u001b[0m | \u001b[0m 0.5452  \u001b[0m | \u001b[0m 3.864   \u001b[0m | \u001b[0m 0.3231  \u001b[0m | \u001b[0m 8.883   \u001b[0m | \u001b[0m 2.041   \u001b[0m | \u001b[0m 464.0   \u001b[0m | \u001b[0m 0.9093  \u001b[0m |\n",
      "| \u001b[0m 63      \u001b[0m | \u001b[0m 0.8306  \u001b[0m | \u001b[0m 0.9815  \u001b[0m | \u001b[0m 0.9252  \u001b[0m | \u001b[0m 0.03044 \u001b[0m | \u001b[0m 8.492   \u001b[0m | \u001b[0m 2.301   \u001b[0m | \u001b[0m 464.7   \u001b[0m | \u001b[0m 0.6417  \u001b[0m |\n",
      "| \u001b[0m 64      \u001b[0m | \u001b[0m 0.8272  \u001b[0m | \u001b[0m 0.5415  \u001b[0m | \u001b[0m 2.058   \u001b[0m | \u001b[0m 0.487   \u001b[0m | \u001b[0m 9.652   \u001b[0m | \u001b[0m 1.874   \u001b[0m | \u001b[0m 480.7   \u001b[0m | \u001b[0m 0.5744  \u001b[0m |\n",
      "| \u001b[0m 65      \u001b[0m | \u001b[0m 0.8294  \u001b[0m | \u001b[0m 0.7706  \u001b[0m | \u001b[0m 3.819   \u001b[0m | \u001b[0m 0.04097 \u001b[0m | \u001b[0m 7.572   \u001b[0m | \u001b[0m 0.3835  \u001b[0m | \u001b[0m 462.9   \u001b[0m | \u001b[0m 0.9622  \u001b[0m |\n",
      "| \u001b[0m 66      \u001b[0m | \u001b[0m 0.8406  \u001b[0m | \u001b[0m 0.6368  \u001b[0m | \u001b[0m 3.221   \u001b[0m | \u001b[0m 0.4197  \u001b[0m | \u001b[0m 8.874   \u001b[0m | \u001b[0m 1.419   \u001b[0m | \u001b[0m 466.3   \u001b[0m | \u001b[0m 0.6814  \u001b[0m |\n",
      "| \u001b[0m 67      \u001b[0m | \u001b[0m 0.8182  \u001b[0m | \u001b[0m 0.9309  \u001b[0m | \u001b[0m 4.187   \u001b[0m | \u001b[0m 0.4859  \u001b[0m | \u001b[0m 9.464   \u001b[0m | \u001b[0m 5.09    \u001b[0m | \u001b[0m 463.7   \u001b[0m | \u001b[0m 0.8244  \u001b[0m |\n",
      "| \u001b[0m 68      \u001b[0m | \u001b[0m 0.8384  \u001b[0m | \u001b[0m 0.7389  \u001b[0m | \u001b[0m 4.517   \u001b[0m | \u001b[0m 0.2263  \u001b[0m | \u001b[0m 9.81    \u001b[0m | \u001b[0m 0.4374  \u001b[0m | \u001b[0m 466.4   \u001b[0m | \u001b[0m 0.9159  \u001b[0m |\n",
      "| \u001b[0m 69      \u001b[0m | \u001b[0m 0.8216  \u001b[0m | \u001b[0m 0.7205  \u001b[0m | \u001b[0m 1.731   \u001b[0m | \u001b[0m 0.3268  \u001b[0m | \u001b[0m 9.398   \u001b[0m | \u001b[0m 0.6617  \u001b[0m | \u001b[0m 464.6   \u001b[0m | \u001b[0m 0.6698  \u001b[0m |\n",
      "| \u001b[0m 70      \u001b[0m | \u001b[0m 0.8305  \u001b[0m | \u001b[0m 0.6577  \u001b[0m | \u001b[0m 4.06    \u001b[0m | \u001b[0m 0.3849  \u001b[0m | \u001b[0m 7.042   \u001b[0m | \u001b[0m 0.1423  \u001b[0m | \u001b[0m 465.6   \u001b[0m | \u001b[0m 0.8062  \u001b[0m |\n",
      "| \u001b[0m 71      \u001b[0m | \u001b[0m 0.8373  \u001b[0m | \u001b[0m 0.8266  \u001b[0m | \u001b[0m 2.571   \u001b[0m | \u001b[0m 0.332   \u001b[0m | \u001b[0m 9.64    \u001b[0m | \u001b[0m 2.361   \u001b[0m | \u001b[0m 467.0   \u001b[0m | \u001b[0m 0.9464  \u001b[0m |\n",
      "| \u001b[0m 72      \u001b[0m | \u001b[0m 0.8283  \u001b[0m | \u001b[0m 0.8513  \u001b[0m | \u001b[0m 4.132   \u001b[0m | \u001b[0m 0.2657  \u001b[0m | \u001b[0m 8.578   \u001b[0m | \u001b[0m 0.7608  \u001b[0m | \u001b[0m 467.5   \u001b[0m | \u001b[0m 0.8415  \u001b[0m |\n",
      "| \u001b[0m 73      \u001b[0m | \u001b[0m 0.8406  \u001b[0m | \u001b[0m 0.5788  \u001b[0m | \u001b[0m 2.071   \u001b[0m | \u001b[0m 0.2544  \u001b[0m | \u001b[0m 7.43    \u001b[0m | \u001b[0m 0.9052  \u001b[0m | \u001b[0m 475.9   \u001b[0m | \u001b[0m 0.7783  \u001b[0m |\n",
      "| \u001b[0m 74      \u001b[0m | \u001b[0m 0.8395  \u001b[0m | \u001b[0m 0.8575  \u001b[0m | \u001b[0m 2.687   \u001b[0m | \u001b[0m 0.04748 \u001b[0m | \u001b[0m 9.765   \u001b[0m | \u001b[0m 2.237   \u001b[0m | \u001b[0m 465.5   \u001b[0m | \u001b[0m 0.7647  \u001b[0m |\n",
      "| \u001b[0m 75      \u001b[0m | \u001b[0m 0.8317  \u001b[0m | \u001b[0m 0.5363  \u001b[0m | \u001b[0m 3.769   \u001b[0m | \u001b[0m 0.02655 \u001b[0m | \u001b[0m 8.044   \u001b[0m | \u001b[0m 2.12    \u001b[0m | \u001b[0m 465.8   \u001b[0m | \u001b[0m 0.7592  \u001b[0m |\n",
      "| \u001b[0m 76      \u001b[0m | \u001b[0m 0.8361  \u001b[0m | \u001b[0m 0.8476  \u001b[0m | \u001b[0m 4.396   \u001b[0m | \u001b[0m 0.2621  \u001b[0m | \u001b[0m 9.874   \u001b[0m | \u001b[0m 1.672   \u001b[0m | \u001b[0m 466.5   \u001b[0m | \u001b[0m 0.8783  \u001b[0m |\n",
      "| \u001b[0m 77      \u001b[0m | \u001b[0m 0.8351  \u001b[0m | \u001b[0m 0.6034  \u001b[0m | \u001b[0m 1.588   \u001b[0m | \u001b[0m 0.3989  \u001b[0m | \u001b[0m 6.388   \u001b[0m | \u001b[0m 0.7422  \u001b[0m | \u001b[0m 476.7   \u001b[0m | \u001b[0m 0.7939  \u001b[0m |\n",
      "| \u001b[0m 78      \u001b[0m | \u001b[0m 0.835   \u001b[0m | \u001b[0m 0.9849  \u001b[0m | \u001b[0m 1.638   \u001b[0m | \u001b[0m 0.0854  \u001b[0m | \u001b[0m 7.337   \u001b[0m | \u001b[0m 0.2113  \u001b[0m | \u001b[0m 475.4   \u001b[0m | \u001b[0m 0.6894  \u001b[0m |\n",
      "| \u001b[0m 79      \u001b[0m | \u001b[0m 0.8294  \u001b[0m | \u001b[0m 0.7422  \u001b[0m | \u001b[0m 2.423   \u001b[0m | \u001b[0m 0.3281  \u001b[0m | \u001b[0m 9.6     \u001b[0m | \u001b[0m 0.2035  \u001b[0m | \u001b[0m 466.9   \u001b[0m | \u001b[0m 0.666   \u001b[0m |\n",
      "| \u001b[0m 80      \u001b[0m | \u001b[0m 0.8294  \u001b[0m | \u001b[0m 0.6354  \u001b[0m | \u001b[0m 4.212   \u001b[0m | \u001b[0m 0.2108  \u001b[0m | \u001b[0m 3.795   \u001b[0m | \u001b[0m 0.1974  \u001b[0m | \u001b[0m 467.6   \u001b[0m | \u001b[0m 0.8598  \u001b[0m |\n",
      "| \u001b[0m 81      \u001b[0m | \u001b[0m 0.8373  \u001b[0m | \u001b[0m 0.7894  \u001b[0m | \u001b[0m 2.642   \u001b[0m | \u001b[0m 0.2067  \u001b[0m | \u001b[0m 8.412   \u001b[0m | \u001b[0m 0.1946  \u001b[0m | \u001b[0m 473.2   \u001b[0m | \u001b[0m 0.7893  \u001b[0m |\n",
      "| \u001b[0m 82      \u001b[0m | \u001b[0m 0.8384  \u001b[0m | \u001b[0m 0.6662  \u001b[0m | \u001b[0m 2.393   \u001b[0m | \u001b[0m 0.3472  \u001b[0m | \u001b[0m 9.854   \u001b[0m | \u001b[0m 1.147   \u001b[0m | \u001b[0m 475.7   \u001b[0m | \u001b[0m 0.6313  \u001b[0m |\n",
      "| \u001b[0m 83      \u001b[0m | \u001b[0m 0.8339  \u001b[0m | \u001b[0m 0.9524  \u001b[0m | \u001b[0m 2.903   \u001b[0m | \u001b[0m 0.2088  \u001b[0m | \u001b[0m 7.701   \u001b[0m | \u001b[0m 2.002   \u001b[0m | \u001b[0m 475.7   \u001b[0m | \u001b[0m 0.9755  \u001b[0m |\n",
      "| \u001b[0m 84      \u001b[0m | \u001b[0m 0.8137  \u001b[0m | \u001b[0m 0.6486  \u001b[0m | \u001b[0m 3.688   \u001b[0m | \u001b[0m 0.3284  \u001b[0m | \u001b[0m 3.54    \u001b[0m | \u001b[0m 3.377   \u001b[0m | \u001b[0m 475.1   \u001b[0m | \u001b[0m 0.8271  \u001b[0m |\n",
      "| \u001b[0m 85      \u001b[0m | \u001b[0m 0.8294  \u001b[0m | \u001b[0m 0.5274  \u001b[0m | \u001b[0m 3.193   \u001b[0m | \u001b[0m 0.06007 \u001b[0m | \u001b[0m 9.223   \u001b[0m | \u001b[0m 1.364   \u001b[0m | \u001b[0m 474.7   \u001b[0m | \u001b[0m 0.6422  \u001b[0m |\n",
      "| \u001b[0m 86      \u001b[0m | \u001b[0m 0.8283  \u001b[0m | \u001b[0m 0.7809  \u001b[0m | \u001b[0m 0.901   \u001b[0m | \u001b[0m 0.1932  \u001b[0m | \u001b[0m 9.611   \u001b[0m | \u001b[0m 3.051   \u001b[0m | \u001b[0m 476.0   \u001b[0m | \u001b[0m 0.6745  \u001b[0m |\n",
      "| \u001b[0m 87      \u001b[0m | \u001b[0m 0.8204  \u001b[0m | \u001b[0m 0.7966  \u001b[0m | \u001b[0m 0.551   \u001b[0m | \u001b[0m 0.1798  \u001b[0m | \u001b[0m 3.7     \u001b[0m | \u001b[0m 7.615   \u001b[0m | \u001b[0m 477.5   \u001b[0m | \u001b[0m 0.7859  \u001b[0m |\n",
      "| \u001b[0m 88      \u001b[0m | \u001b[0m 0.8294  \u001b[0m | \u001b[0m 0.5229  \u001b[0m | \u001b[0m 4.861   \u001b[0m | \u001b[0m 0.328   \u001b[0m | \u001b[0m 9.963   \u001b[0m | \u001b[0m 0.5502  \u001b[0m | \u001b[0m 480.7   \u001b[0m | \u001b[0m 0.5716  \u001b[0m |\n",
      "| \u001b[95m 89      \u001b[0m | \u001b[95m 0.8429  \u001b[0m | \u001b[95m 0.618   \u001b[0m | \u001b[95m 1.478   \u001b[0m | \u001b[95m 0.1827  \u001b[0m | \u001b[95m 9.229   \u001b[0m | \u001b[95m 0.5475  \u001b[0m | \u001b[95m 475.7   \u001b[0m | \u001b[95m 0.6338  \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 90      \u001b[0m | \u001b[0m 0.8339  \u001b[0m | \u001b[0m 0.7842  \u001b[0m | \u001b[0m 2.114   \u001b[0m | \u001b[0m 0.05026 \u001b[0m | \u001b[0m 5.753   \u001b[0m | \u001b[0m 0.05312 \u001b[0m | \u001b[0m 480.8   \u001b[0m | \u001b[0m 0.7416  \u001b[0m |\n",
      "| \u001b[0m 91      \u001b[0m | \u001b[0m 0.8373  \u001b[0m | \u001b[0m 0.6813  \u001b[0m | \u001b[0m 1.834   \u001b[0m | \u001b[0m 0.07373 \u001b[0m | \u001b[0m 9.846   \u001b[0m | \u001b[0m 0.002596\u001b[0m | \u001b[0m 475.1   \u001b[0m | \u001b[0m 0.5406  \u001b[0m |\n",
      "| \u001b[0m 92      \u001b[0m | \u001b[0m 0.8283  \u001b[0m | \u001b[0m 0.9437  \u001b[0m | \u001b[0m 1.792   \u001b[0m | \u001b[0m 0.1399  \u001b[0m | \u001b[0m 3.542   \u001b[0m | \u001b[0m 1.557   \u001b[0m | \u001b[0m 483.7   \u001b[0m | \u001b[0m 0.555   \u001b[0m |\n",
      "| \u001b[0m 93      \u001b[0m | \u001b[0m 0.8395  \u001b[0m | \u001b[0m 0.9708  \u001b[0m | \u001b[0m 2.89    \u001b[0m | \u001b[0m 0.4921  \u001b[0m | \u001b[0m 8.004   \u001b[0m | \u001b[0m 0.3236  \u001b[0m | \u001b[0m 476.6   \u001b[0m | \u001b[0m 0.8873  \u001b[0m |\n",
      "| \u001b[0m 94      \u001b[0m | \u001b[0m 0.8272  \u001b[0m | \u001b[0m 0.7352  \u001b[0m | \u001b[0m 4.36    \u001b[0m | \u001b[0m 0.3405  \u001b[0m | \u001b[0m 3.157   \u001b[0m | \u001b[0m 2.333   \u001b[0m | \u001b[0m 764.5   \u001b[0m | \u001b[0m 0.5638  \u001b[0m |\n",
      "| \u001b[0m 95      \u001b[0m | \u001b[0m 0.8361  \u001b[0m | \u001b[0m 0.5369  \u001b[0m | \u001b[0m 2.457   \u001b[0m | \u001b[0m 0.1199  \u001b[0m | \u001b[0m 8.582   \u001b[0m | \u001b[0m 0.2166  \u001b[0m | \u001b[0m 477.0   \u001b[0m | \u001b[0m 0.7171  \u001b[0m |\n",
      "| \u001b[0m 96      \u001b[0m | \u001b[0m 0.8227  \u001b[0m | \u001b[0m 0.9032  \u001b[0m | \u001b[0m 1.125   \u001b[0m | \u001b[0m 0.1848  \u001b[0m | \u001b[0m 9.942   \u001b[0m | \u001b[0m 5.232   \u001b[0m | \u001b[0m 616.0   \u001b[0m | \u001b[0m 0.6426  \u001b[0m |\n",
      "| \u001b[0m 97      \u001b[0m | \u001b[0m 0.8272  \u001b[0m | \u001b[0m 0.9138  \u001b[0m | \u001b[0m 0.27    \u001b[0m | \u001b[0m 0.05426 \u001b[0m | \u001b[0m 5.266   \u001b[0m | \u001b[0m 3.967   \u001b[0m | \u001b[0m 881.0   \u001b[0m | \u001b[0m 0.6343  \u001b[0m |\n",
      "| \u001b[0m 98      \u001b[0m | \u001b[0m 0.8283  \u001b[0m | \u001b[0m 0.6908  \u001b[0m | \u001b[0m 3.41    \u001b[0m | \u001b[0m 0.3632  \u001b[0m | \u001b[0m 6.724   \u001b[0m | \u001b[0m 4.353   \u001b[0m | \u001b[0m 278.5   \u001b[0m | \u001b[0m 0.5271  \u001b[0m |\n",
      "| \u001b[0m 99      \u001b[0m | \u001b[0m 0.835   \u001b[0m | \u001b[0m 0.5564  \u001b[0m | \u001b[0m 3.264   \u001b[0m | \u001b[0m 0.3877  \u001b[0m | \u001b[0m 6.08    \u001b[0m | \u001b[0m 0.09319 \u001b[0m | \u001b[0m 473.8   \u001b[0m | \u001b[0m 0.9094  \u001b[0m |\n",
      "| \u001b[0m 100     \u001b[0m | \u001b[0m 0.8339  \u001b[0m | \u001b[0m 0.5714  \u001b[0m | \u001b[0m 2.721   \u001b[0m | \u001b[0m 0.2871  \u001b[0m | \u001b[0m 5.085   \u001b[0m | \u001b[0m 4.823   \u001b[0m | \u001b[0m 430.3   \u001b[0m | \u001b[0m 0.7606  \u001b[0m |\n",
      "| \u001b[0m 101     \u001b[0m | \u001b[0m 0.8283  \u001b[0m | \u001b[0m 0.5281  \u001b[0m | \u001b[0m 4.065   \u001b[0m | \u001b[0m 0.1962  \u001b[0m | \u001b[0m 6.74    \u001b[0m | \u001b[0m 4.297   \u001b[0m | \u001b[0m 428.7   \u001b[0m | \u001b[0m 0.9939  \u001b[0m |\n",
      "| \u001b[0m 102     \u001b[0m | \u001b[0m 0.8316  \u001b[0m | \u001b[0m 0.5422  \u001b[0m | \u001b[0m 4.62    \u001b[0m | \u001b[0m 0.4896  \u001b[0m | \u001b[0m 9.615   \u001b[0m | \u001b[0m 0.001558\u001b[0m | \u001b[0m 465.0   \u001b[0m | \u001b[0m 0.9908  \u001b[0m |\n",
      "| \u001b[0m 103     \u001b[0m | \u001b[0m 0.835   \u001b[0m | \u001b[0m 0.5246  \u001b[0m | \u001b[0m 0.7975  \u001b[0m | \u001b[0m 0.01901 \u001b[0m | \u001b[0m 4.899   \u001b[0m | \u001b[0m 5.703   \u001b[0m | \u001b[0m 431.8   \u001b[0m | \u001b[0m 0.8009  \u001b[0m |\n",
      "| \u001b[0m 104     \u001b[0m | \u001b[0m 0.8249  \u001b[0m | \u001b[0m 0.5289  \u001b[0m | \u001b[0m 1.092   \u001b[0m | \u001b[0m 0.473   \u001b[0m | \u001b[0m 3.25    \u001b[0m | \u001b[0m 5.068   \u001b[0m | \u001b[0m 429.9   \u001b[0m | \u001b[0m 0.6777  \u001b[0m |\n",
      "| \u001b[0m 105     \u001b[0m | \u001b[0m 0.8317  \u001b[0m | \u001b[0m 0.7338  \u001b[0m | \u001b[0m 2.042   \u001b[0m | \u001b[0m 0.05324 \u001b[0m | \u001b[0m 4.884   \u001b[0m | \u001b[0m 4.319   \u001b[0m | \u001b[0m 433.2   \u001b[0m | \u001b[0m 0.7684  \u001b[0m |\n",
      "| \u001b[0m 106     \u001b[0m | \u001b[0m 0.8137  \u001b[0m | \u001b[0m 0.6066  \u001b[0m | \u001b[0m 3.022   \u001b[0m | \u001b[0m 0.02419 \u001b[0m | \u001b[0m 4.602   \u001b[0m | \u001b[0m 6.528   \u001b[0m | \u001b[0m 432.9   \u001b[0m | \u001b[0m 0.6201  \u001b[0m |\n",
      "| \u001b[0m 107     \u001b[0m | \u001b[0m 0.8216  \u001b[0m | \u001b[0m 0.653   \u001b[0m | \u001b[0m 0.6368  \u001b[0m | \u001b[0m 0.453   \u001b[0m | \u001b[0m 5.171   \u001b[0m | \u001b[0m 2.428   \u001b[0m | \u001b[0m 431.6   \u001b[0m | \u001b[0m 0.7712  \u001b[0m |\n",
      "| \u001b[0m 108     \u001b[0m | \u001b[0m 0.8205  \u001b[0m | \u001b[0m 0.8856  \u001b[0m | \u001b[0m 1.006   \u001b[0m | \u001b[0m 0.07232 \u001b[0m | \u001b[0m 6.255   \u001b[0m | \u001b[0m 6.423   \u001b[0m | \u001b[0m 430.1   \u001b[0m | \u001b[0m 0.5757  \u001b[0m |\n",
      "| \u001b[0m 109     \u001b[0m | \u001b[0m 0.835   \u001b[0m | \u001b[0m 0.8435  \u001b[0m | \u001b[0m 0.669   \u001b[0m | \u001b[0m 0.129   \u001b[0m | \u001b[0m 6.273   \u001b[0m | \u001b[0m 0.3551  \u001b[0m | \u001b[0m 470.1   \u001b[0m | \u001b[0m 0.6434  \u001b[0m |\n",
      "| \u001b[0m 110     \u001b[0m | \u001b[0m 0.8294  \u001b[0m | \u001b[0m 0.7523  \u001b[0m | \u001b[0m 0.5295  \u001b[0m | \u001b[0m 0.1032  \u001b[0m | \u001b[0m 9.718   \u001b[0m | \u001b[0m 4.436   \u001b[0m | \u001b[0m 467.9   \u001b[0m | \u001b[0m 0.8057  \u001b[0m |\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pbounds = {  \n",
    "    'learning_rate': (0.01, 0.5),  \n",
    "    'n_estimators': (100, 1000), \n",
    "    'max_depth': (3, 10),\n",
    "    'min_child_weight': (0, 10),\n",
    "    'subsample': (0.5, 1.0),  \n",
    "    'colsample_bytree': (0.5, 1.0),   \n",
    "    'gamma': (0, 5)\n",
    "    # 'reg_lambda': (0, 1000, 'log-uniform'),\n",
    "    # 'reg_alpha': (0, 1.0, 'log-uniform')    \n",
    "}\n",
    "\n",
    "def xgboost_hyper_param(learning_rate, n_estimators, max_depth, min_child_weight, subsample, colsample_bytree, gamma):\n",
    "    max_depth = int(max_depth)\n",
    "    n_estimators = int(n_estimators)\n",
    "    clf = XGBClassifier(\n",
    "        max_depth=max_depth, \n",
    "        min_child_weight= min_child_weight,\n",
    "        learning_rate=learning_rate, \n",
    "        n_estimators=n_estimators, \n",
    "        subsample=subsample, \n",
    "        colsample_bytree=colsample_bytree, \n",
    "        gamma=gamma,\n",
    "        random_state=1,\n",
    "        eval_metric='logloss'\n",
    "        # reg_alpha=reg_alpha,\n",
    "        # reg_lambda=reg_lambda\n",
    "    )\n",
    "    return np.mean(cross_val_score(clf, train_importance, train_answer, cv=5, scoring='accuracy'))\n",
    "\n",
    "optimizer = BayesianOptimization( f=xgboost_hyper_param, pbounds=pbounds, random_state=1)\n",
    "# init_points: 초기 랜덤 포인트 갯수\n",
    "# acq='ei': Expected Improvement \n",
    "# xi=0.01: exploration(불확실성이 가장 높은 점 근처에 최적값이 존재할 것이라는 가정으로 계산된 값) 강도 (보통 0.01) \n",
    "optimizer.maximize(init_points=10, n_iter=100, acq='ei', xi=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 0.8429037725189881,\n",
       " 'params': {'colsample_bytree': 0.6179546440577699,\n",
       "  'gamma': 1.477720800896523,\n",
       "  'learning_rate': 0.1826603868194322,\n",
       "  'max_depth': 9.228707673078954,\n",
       "  'min_child_weight': 0.5474634306612236,\n",
       "  'n_estimators': 475.74577417793563,\n",
       "  'subsample': 0.6338104141514135}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #E65100;background-color:#FFF3E0;padding:10px\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#BF360C;\">큰그림으로 이해하기</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">데이터를 어떻게 짜르느냐에 따라, 예측 성능은 다르게 계산될 수 있지만,</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">동일한 테스트 환경에서, 하이퍼파라미터 튜닝 전, XGBoost 는 약 82.26% 였지만, 튜닝 후, 84.51% 까지 예측 정확도가 올라감</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization LightGBM 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | learni... | max_depth | min_ch... | n_esti... | subsample |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.8283  \u001b[0m | \u001b[95m 0.6439  \u001b[0m | \u001b[95m 0.07371 \u001b[0m | \u001b[95m 3.136   \u001b[0m | \u001b[95m 6.788   \u001b[0m | \u001b[95m 290.5   \u001b[0m | \u001b[95m 0.6328  \u001b[0m |\n",
      "| \u001b[95m 18      \u001b[0m | \u001b[95m 0.8294  \u001b[0m | \u001b[95m 0.881   \u001b[0m | \u001b[95m 0.02824 \u001b[0m | \u001b[95m 6.011   \u001b[0m | \u001b[95m 5.882   \u001b[0m | \u001b[95m 222.4   \u001b[0m | \u001b[95m 0.9225  \u001b[0m |\n",
      "| \u001b[95m 66      \u001b[0m | \u001b[95m 0.8328  \u001b[0m | \u001b[95m 0.5421  \u001b[0m | \u001b[95m 0.2023  \u001b[0m | \u001b[95m 7.472   \u001b[0m | \u001b[95m 6.72    \u001b[0m | \u001b[95m 122.9   \u001b[0m | \u001b[95m 0.75    \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pbounds = {  \n",
    "    'learning_rate': (0.01, 0.5),  \n",
    "    'n_estimators': (100, 1000), \n",
    "    'max_depth': (3, 10),\n",
    "    'min_child_weight': (0, 10),    \n",
    "    'subsample': (0.5, 1.0),\n",
    "    'colsample_bytree': (0.5, 1.0)\n",
    "    # 'reg_lambda': (0, 1000),\n",
    "    # 'reg_alpha': (0, 1.0)\n",
    "}\n",
    "\n",
    "def lgbm_hyper_param(learning_rate, n_estimators, max_depth, min_child_weight, subsample, colsample_bytree):\n",
    "    max_depth = int(max_depth)\n",
    "    n_estimators = int(n_estimators)\n",
    "    clf = LGBMClassifier(\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=min_child_weight,\n",
    "        learning_rate=learning_rate, \n",
    "        n_estimators=n_estimators, \n",
    "        subsample=subsample, \n",
    "        colsample_bytree=colsample_bytree,\n",
    "        random_state=1\n",
    "        # reg_lambda=reg_lambda,        \n",
    "        # reg_alpha=reg_alpha\n",
    "    )\n",
    "    return np.mean(cross_val_score(clf, train_importance, train_answer, cv=5, scoring='accuracy'))   # cv 도 숫자로 작성하여, 내부적으로 (Stratified)KFold 사용함\n",
    "\n",
    "optimizer = BayesianOptimization( f=lgbm_hyper_param, pbounds=pbounds, verbose=1, random_state=1)\n",
    "optimizer.maximize(init_points=10, n_iter=100, acq='ei', xi=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 0.832772581758835,\n",
       " 'params': {'colsample_bytree': 0.5421003116502919,\n",
       "  'learning_rate': 0.2023046532609743,\n",
       "  'max_depth': 7.472129677653835,\n",
       "  'min_child_weight': 6.720185321453051,\n",
       "  'n_estimators': 122.93072230756829,\n",
       "  'subsample': 0.7500265897729492}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #E65100;background-color:#FFF3E0;padding:10px\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#BF360C;\">큰그림으로 이해하기</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">LightGBM 은 수행시간은 XGBoost 보다 단축되고, </font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">예측 정확도는 거의 유사하거나, 살짝 낮은 정도임을 확인할 수 있으므로, 많은 테스트를 위해서는 LightGBM을 사용하는 것도 좋음</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Grid Search XGBoost 적용\n",
    "\n",
    "### Grid Search XGBoost 적용 1단계\n",
    "- 주요 파라미터를 모두 넣을 경우, 수행시간이 매우 길어지므로, 3단계로 나누어서 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  3.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8428598330299417\n",
      "{'learning_rate': 0.17, 'n_estimators': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  5.1min finished\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.001, 0.005, 0.01, 0.05, 0.06, 0.1, 0.12, 0.15, 0.17, 0.2]\n",
    "n_estimators = [10, 50, 60, 75, 85, 100, 125, 150, 200, 250, 500, 1000]\n",
    "\n",
    "hyperparams = {\n",
    "    'learning_rate': learning_rate, \n",
    "    'n_estimators': n_estimators\n",
    "}\n",
    "\n",
    "gd=GridSearchCV(\n",
    "    estimator = XGBClassifier(random_state=1, eval_metric='logloss'), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5, \n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gd.fit(train_importance, train_answer)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search XGBoost 적용 2단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 249 out of 280 | elapsed:    5.2s remaining:    0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8428598330299417\n",
      "{'max_depth': 6, 'min_child_weight': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed:    5.8s finished\n"
     ]
    }
   ],
   "source": [
    "max_depth = [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "min_child_weight = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "hyperparams = {\n",
    "    'max_depth': max_depth, \n",
    "    'min_child_weight': min_child_weight\n",
    "}\n",
    "\n",
    "gd=GridSearchCV(\n",
    "    estimator = XGBClassifier(learning_rate=0.17, n_estimators=10, random_state=1, eval_metric='logloss'), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5, \n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gd.fit(train_importance, train_answer)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search XGBoost 적용 3단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2025 candidates, totalling 10125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:   25.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed:   37.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed:   51.9s\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4018 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4968 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6018 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7168 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 8418 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 9768 tasks      | elapsed:  3.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8439834285355596\n",
      "{'colsample_bytree': 0.85, 'gamma': 0.2, 'reg_alpha': 0.01, 'subsample': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 10125 out of 10125 | elapsed:  3.8min finished\n"
     ]
    }
   ],
   "source": [
    "gamma =  [i*0.1 for i in range(0,5)]\n",
    "subsample = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "colsample_bytree = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "reg_alpha = [1e-5, 1e-2, 0.1, 1, 100]\n",
    "\n",
    "hyperparams = {\n",
    "    'gamma': gamma,\n",
    "    'subsample':subsample,\n",
    "    'colsample_bytree':colsample_bytree,\n",
    "    'reg_alpha': reg_alpha\n",
    "}\n",
    "\n",
    "gd=GridSearchCV(\n",
    "    estimator = XGBClassifier(\n",
    "        learning_rate=0.17, \n",
    "        n_estimators=10, \n",
    "        max_depth=6, \n",
    "        min_child_weight=1,\n",
    "        random_state=1,\n",
    "        eval_metric='logloss'\n",
    "    ), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5, \n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gd.fit(train_importance, train_answer)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #E65100;background-color:#FFF3E0;padding:10px\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#BF360C;\">큰그림으로 이해하기</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">XGBoost 를 Grid Search 에 적용할 때, </font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">사전에 여러 테스트를 통해, 파라미터값에 대한 대략적인 감을 가진다면, 보다 좋은 성능 값을 찾아낼 수도 있음</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8350323269097985\n",
      "{'n_neighbors': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "hyperparams = { \n",
    "    'n_neighbors': n_neighbors\n",
    "}\n",
    "\n",
    "gd=GridSearchCV(\n",
    "    estimator = KNeighborsClassifier(), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5, \n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gd.fit(train_importance, train_answer)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1200 candidates, totalling 6000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 804 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2404 tasks      | elapsed:   35.4s\n",
      "[Parallel(n_jobs=-1)]: Done 3504 tasks      | elapsed:   52.4s\n",
      "[Parallel(n_jobs=-1)]: Done 4804 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5969 out of 6000 | elapsed:  1.6min remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 6000 out of 6000 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8484840876278952\n",
      "{'max_depth': None, 'max_features': 0.8, 'min_samples_leaf': 2, 'min_samples_split': 6, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [10, 50, 100, 200]\n",
    "max_depth = [3, None] \n",
    "max_features = [0.1, 0.2, 0.5, 0.8, 'sqrt', 'log2'] # feature 수\n",
    "min_samples_split = [2, 4, 6, 8, 10] # 노드를 분할하기 위한 최소 샘플 수\n",
    "min_samples_leaf = [2, 4, 6, 8, 10] # 리프 노드가 되기 위해 필요한 최소 샘플 수\n",
    "\n",
    "hyperparams = {\n",
    "    'n_estimators': n_estimators, \n",
    "    'max_depth': max_depth, \n",
    "    'max_features': max_features,\n",
    "    'min_samples_split': min_samples_split, \n",
    "    'min_samples_leaf': min_samples_leaf\n",
    "}\n",
    "\n",
    "gd=GridSearchCV(\n",
    "    estimator = RandomForestClassifier(random_state=1), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5, \n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gd.fit(train_importance, train_answer)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1200 candidates, totalling 6000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 512 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1512 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=-1)]: Done 2912 tasks      | elapsed:   32.8s\n",
      "[Parallel(n_jobs=-1)]: Done 4082 tasks      | elapsed:   48.3s\n",
      "[Parallel(n_jobs=-1)]: Done 4632 tasks      | elapsed:   59.3s\n",
      "[Parallel(n_jobs=-1)]: Done 5282 tasks      | elapsed:  1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8552068294520119\n",
      "{'max_depth': None, 'max_features': 0.5, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 6000 out of 6000 | elapsed:  1.4min finished\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [10, 50, 100, 200]\n",
    "max_depth = [3, None] \n",
    "max_features = [0.1, 0.2, 0.5, 0.8, 'sqrt', 'log2'] # feature 수\n",
    "min_samples_split = [2, 4, 6, 8, 10] # 노드를 분할하기 위한 최소 샘플 수\n",
    "min_samples_leaf = [2, 4, 6, 8, 10] # 리프 노드가 되기 위해 필요한 최소 샘플 수\n",
    "\n",
    "hyperparams = {\n",
    "    'n_estimators': n_estimators, \n",
    "    'max_depth': max_depth, \n",
    "    'max_features': max_features,\n",
    "    'min_samples_split': min_samples_split, \n",
    "    'min_samples_leaf': min_samples_leaf\n",
    "}\n",
    "\n",
    "gd=GridSearchCV(\n",
    "    estimator = ExtraTreesClassifier(random_state=1), \n",
    "    param_grid = hyperparams, \n",
    "    verbose=True, \n",
    "    cv=5, \n",
    "    scoring = \"accuracy\", \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gd.fit(train_importance, train_answer)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 2px solid #E65100;background-color:#FFF3E0;padding:10px\">\n",
    "<font size=\"4em\" style=\"font-weight:bold;color:#BF360C;\">큰그림으로 이해하기</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">훈련 셋을 랜덤하게 분리하므로, 예측 성능은 조금씩 차이는 있을 수 있지만,</font><br>\n",
    "<font size=\"4em\" style=\"color:#BF360C;\">하이퍼파라미터 튜닝 후, 개선된 성능을 확인할 수 있음</font><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"border: 1px solid #455A64;background-color:#ECEFF1;\">\n",
    "본 자료 및 영상 컨텐츠는 저작권법 제25조 2항에 의해 보호를 받습니다. 본 컨텐츠 및 컨텐츠 일부 문구등을 외부에 공개, 게시하는 것을 금지합니다. 특히 자료에 대해서는 저작권법을 엄격하게 적용하겠습니다.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
